---
title: "Determining a Fair Apartment Price"
subtitle: "Stat 410 Final Project"
author: "Jack Dzialo, Devin Abraham, Tom Vincent"
format:
  revealjs:
    theme: ./_extensions/grantmcdermott/clean/clean.scss
    transition: fade
---

## Purpose

### Why choose our dataset?

::: {.incremental}
- Aimed to address an [actual]{.bg style="--col: #D0E1F9"} problem that we face
- Currently looking for housing next year
:::

::: {.fragment .r-fit-text}
[Challenge]{.bg style="--col: #D0E1F9"}: How to determine if price is fair?
:::

## Proposal

::: {.incremental}
- Chose to use [Forward Stepwise Search]{.bg style="--col: #D0E1F9"}, using F tests
  - Tested interaction terms once individual predictors were determined
- Read up upon different ways to determine model accuracy
- Use confidence intervals generated by the model to see if posted price for an apartment is fair
:::


::: {.fragment}
::: {.callout-tip}
## Definitions
1. **AIC - **Measures trade off between model fit and complexity with a penalty for more complexity
2. **AICc - **Corrected version of AIC with a larger penalty for regressors
3. **BIC - ** Penalizes model complexity heavily and favors simple model with large sample sizes
4. **PRESS - ** Predicted Residual Error Sum of Squares; predicts how accurate a model is at predicting data it hasn't seen before
:::
:::

## Data Collection

::: {.incremental}
- Dataset of apartments for rent, sourced from [UCI dataset repository](https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified)
- [52,000]{.bg style="--col: #D0E1F9"} rows, each representing a different apartment
- [22]{.bg style="--col: #D0E1F9"} columns, each representing a descriptor for an apartment listing
  - Square feet, apartment type, number of bathrooms, has photos
:::
::: {.fragment}
::: {.columns}

::: {.column width="50%"}
```{r fig.width=8, fig.height=6}
data <- read.csv2("apartments.csv")
data$price <- as.numeric(data$price)
hist(data$price,
     freq = FALSE,
     breaks = "scott",
     main = "Histogram of Price",
     xlab = "Price (dollars)",
     col = "lightblue",
     xlim = c(0, 10000))
```
:::
::: {.column width="50%"}
::: {.centered}
```{r}
table <- data.frame(
  Statistic = c("Mean", "Max", "Min", "Median"),
  Value = c(round(mean(data$price, na.rm = TRUE)),
            round(max(data$price, na.rm = TRUE)),
            round(min(data$price, na.rm = TRUE)),
            round(median(data$price, na.rm = TRUE))
))
knitr::kable(table)
```
:::
:::
:::
:::

## Data Collection

The plot below shows each of the continuous predictors plotted against each other, in order to determine if [multicollinearity]{.bg style="--col: #D0E1F9"} exists.

![](410pairs.png){fig-align="center"}

# [Data Analysis]{.bg style="--col: #3929E9"} - Model Building

## Forward Stepwise Search

### First Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the first iteration of our stepwise search, we start off with a linear model that only consists of an intercept, $\beta_0$.
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the basic model with every predictor, we find that the [Square Feet]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.
```{r}
library(ggplot2)
library(data.table)
library(dplyr)
library(stringr)
library(car)
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo",
                    "pets_allowed", "square_feet",
                    "population", "density")
new <- regression(regressor_list)
table <- data.frame(
  Statistic = c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS"), Values = new)
knitr::kable(table)
```
:::
:::
:::

## Forward Stepwise Search {auto-animate=true}

### Second Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the second iteration of our stepwise search, we use the linear model from the previous iteration, being the model with an intercept $\beta_0$ and a single predictor [Square Feet]{.bg style="--col: #D0E1F9"}, denoted as $X_1$.
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0 + X_1\beta_1
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the model with every predictor, we find that the [Density]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.

![](second_iteration.png)

:::
:::
:::

## Forward Stepwise Search {auto-animate=true .smaller}

### Second Iteration

::: {.columns}
::: {.column width="50%"}
**Current:**\
![](second_iteration.png){ width=80% }
:::
::: {.column width="50%"}
::: {.fragment .fade-left}
**Previous:**\
![](first_iteration.png){ width=80% }
:::
:::
:::

::: {.fragment .r-fit-text}
The model improved when including [density]{.bg style="--col: #D0E1F9"}, while **AIC**, **AICc**, **BIC**, **PRESS** got smaller.
:::

## Forward Stepwise Search {auto-animate=true}

### Third Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the third iteration, we now include both predictors: [Square Feet]{.bg style="--col: #D0E1F9"} (denoted by $X_1$), and [density]{.bg style="--col: #D0E1F9"} (denoted by $X_2$).
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the model with every predictor, we find that the [Population]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.

![](third_iteration.png)

:::
:::
:::

## Forward Stepwise Search {auto-animate=true}

### Third Iteration

::: {.columns}
::: {.column width="50%"}
**Current:**\
![](third_iteration.png){ width=80% }
:::
::: {.column width="50%"}
::: {.fragment .fade-left}
**Previous:**\
![](second_iteration.png){ width=80% }
:::
:::
:::

::: {.fragment}
::: {.center style="text-align: center;"}
[Negligible]{.bg style="--col: #D0E1F9"} improvement
:::
:::

## Forward Stepwise Search

### Completed

::: {.fragment}
Hence, we have found our linear model, with the predictor variables being [Square Feet]{.bg style="--col: #D0E1F9"} ($X_1$), and [density]{.bg style="--col: #D0E1F9"} ($X_2$).

$$
Y=\beta_0 + X_1\beta_1 + X_2\beta_2
$$
:::
::: {.fragment}
::: {.columns}
::: {.column width="40%"}
::: {.center style="text-align: center;"}
[32%]{.bg style="--col: #D0E1F9"} of the variation in price explained by model
:::
:::
:::{.column width="60%"}
```{r}
cleaned$price1 <- log(cleaned$price)
model <- lm(price ~ square_feet + density, data=cleaned)
summary_model <- summary(model)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```

:::
:::
:::

# [Analysis of Residuals]{.bg style="--col: #3929E9"} - Model Improvement

## Residuals

### Residuals vs. Fitted Values
:::{.center style="text-align: center;"}

```{r}
model <- lm(price ~ square_feet + density, data=cleaned)
plot(model, which = 1)
```

Slight [heteroskedasticity]{.bg style="--col: #D0E1F9"}
:::

## Residuals

### Quantile-Quantile

:::{.center style="text-align: center;"}
```{r}
plot(model, which = 2)
```
Relatively normal, room for improvement
:::

## Residuals

### Scale-Location

:::{.center style="text-align: center;"}
```{r}
plot(model, which = 3)
```

Another heteroskedasticity indicator
:::


## Residuals

### Residuals vs. Leverage

:::{.center style="text-align: center;"}
```{r}
plot(model, which = 5)
```
Multiple high leverage points
:::

## Adjusted Model

### Outliers and Data Transformation

::: {.columns}
::: {.column width="30%"}
::: {.fragment}
Address non-normality through [Box-Cox]{.bg style="--col: #D0E1F9"} transformation
:::

:::
::: {.column width="70%"}
::: {.fragment}
```{r}
boxCox(model)
```
:::
:::
:::

::: {.columns}
::: {.column width="40%"}
::: {.fragment}
- Optimal $\lambda$ close to 0
:::
:::

::: {.column width="60%"}
::: {.fragment}
$$
y(\lambda) =
\begin{cases}
\frac{y^\lambda - 1}{\lambda}, & \text{if } \lambda \neq 0 \\[10pt]
\ln(y), & \text{if } \lambda = 0
\end{cases}
$$
:::
:::
:::

## Adjusted Model

### Summary

Our adjusted model is: $\text{ln}(Y)=\beta_0+X_1\beta_1+X_2\beta_2$

```{r}
cleaned_new <- cleaned[-c(6754, 20556, 18779, 29745), ]
cleaned_new1 <- cleaned_new[-15668, ]
model_new <- lm(log(price) ~ square_feet + density, data=cleaned_new1)
summary_model <- summary(model_new)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```

::: {.fragment}
::: {.center style="text-align: center;"}
[Improved]{.bg style="--col: #D0E1F9"} $R^2$
:::
:::


## Adjusted Model Residuals

### Residuals vs. Fitted Values


::: {.columns}
::: {.column width=50%}
**Previous**
```{r fig.height=8, fig.width=10}
plot(model, which = 1)
```

:::
::: {.column width=50%}
**Adjusted**
```{r fig.height=8, fig.width=10}
plot(model_new, which = 1)
```
:::
:::
:::{.center style="text-align: center;"}
Normallity improved slightly
:::


## Adjusted Model Residuals

### Quantile-Quantile

::: {.columns}
::: {.column width=50%}
**Previous**
```{r fig.height=8, fig.width=10}
plot(model, which = 2)
```

:::
::: {.column width=50%}
**Adjusted**
```{r fig.height=8, fig.width=10}
plot(model_new, which = 2)
```
:::
:::

:::{.center style="text-align: center;"}
[Significant]{.bg style="--col: #D0E1F9"} improvement
:::


## Adjusted Model Residuals

### Scale-Location

::: {.columns}
::: {.column width=50%}
**Previous**
```{r fig.height=8, fig.width=10}
plot(model, which = 3)
```

:::
::: {.column width=50%}
**Adjusted**
```{r fig.height=8, fig.width=10}
plot(model_new, which = 3)
```
:::
:::
:::{.center style="text-align: center;"}
Slight homoscedasticity
:::


## Adjusted Model Residuals

### Residuals vs. Leverage

::: {.columns}
::: {.column width=50%}
**Previous**
```{r fig.height=8, fig.width=10}
plot(model, which = 5)
```

:::
::: {.column width=50%}
**Adjusted**
```{r fig.height=8, fig.width=10}
plot(model_new, which = 5)
```
:::
:::

:::{.center style="text-align: center;"}
Less high leverage points/outliers
:::


## Confidence Interval

### Real Apartment Listing


::: {.columns}
::: {.column width=50%}
::: {.fragment}
![](apartment_photo.png)
:::
:::
::: {.column width="50%"}
::: {.fragment}
The confidence interval generated by the model:
```{r}
calculated_density <- cities %>% filter(city == "houston", state_id == "TX") %>% select(density)
predictions <- predict(model_new, data.frame(square_feet = c(1200), density = c(2*calculated_density)), interval = "confidence", level = 0.95)
knitr::kable(exp(predictions))
```
::: {.fragment .r-fit-text}
 - Density metric was taken from the dataset
   - Scaled to account for higher density
:::
:::
:::
:::

::: {.fragment}
::: {.center style="text-align: center;"}
[Good prediction!]{.bg style="--col: #D0E1F9"}
:::
:::


## Summary

Talk about what ways we can improve the model in the future.

# [Thank you]{.bg style="--col: #3929E9"} for listening!
