---
title: "Determining a Fair Apartment Price"
subtitle: "Stat 410 Final Project"
author: "Jack Dzialo, Devin Abraham, Tom Vincent"
format:
  revealjs:
    theme: ./_extensions/grantmcdermott/clean/clean.scss
    transition: fade
---

## Purpose

### Why choose our dataset?

TODO: CI of sample house around Rice

::: {.incremental}
- Aimed to address an [actual]{.bg style="--col: #D0E1F9"} problem that we face
- Currently looking for housing next year
:::

::: {.fragment .r-fit-text}
[Challenge]{.bg style="--col: #D0E1F9"}: How to determine if price is fair?
:::

## Proposal

::: {.incremental}
- Chose to use [Forward Stepwise Search]{.bg style="--col: #D0E1F9"}, using F tests
  - Tested interaction terms once individual predictors were determined
- Read up upon different ways to determine model accuracy
- Use confidence intervals generated by the model to see if posted price for an apartment is fair
:::


::: {.fragment}
::: {.callout-tip}
## Definitions
1. **AIC - **
2. **AICc - **
3. **BIC - ** hello
4. **PRESS - ** Predicted Residual Error Sum of Squares; predicts how accurate a model is at predicting data it hasn't seen before
:::
:::

## Data Collection

::: {.incremental}
- Dataset of apartments for rent, sourced from [UCI dataset repository]{https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified}
- [100,000]{.bg style="--col: #D0E1F9"} rows, each representing a different apartment
- [22]{.bg style="--col: #D0E1F9"} columns, each representing a descriptor for an apartment listing
  - Square feet, apartment type, number of bathrooms, has photos
:::
::: {.fragment}
::: {.columns}

::: {.column width="50%"}
```{r fig.width=8, fig.height=6}
data <- read.csv2("apartments.csv")
data$price <- as.numeric(data$price)
hist(data$price,
     freq = FALSE,
     breaks = "scott",
     main = "Histogram of Price",
     xlab = "Price (dollars)",
     col = "lightblue",
     xlim = c(0, 10000))
```
:::
::: {.column width="50%"}
::: {.centered}
```{r}
table <- data.frame(
  Statistic = c("Mean", "Max", "Min", "Median"),
  Value = c(round(mean(data$price, na.rm = TRUE)),
            round(max(data$price, na.rm = TRUE)),
            round(min(data$price, na.rm = TRUE)),
            round(median(data$price, na.rm = TRUE))
))
knitr::kable(table)
```
:::
:::
:::
:::

## Data Collection

The plot below shows each of the continuous predictors plotted against each other, in order to determine if [multicollinearity]{.bg style="--col: #D0E1F9"} exists.

![](410pairs.png){fig-align="center"}

# [Data Analysis]{.bg style="--col: #3929E9"} - Model Building

## Forward Stepwise Search

### First Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the first iteration of our stepwise search, we start off with a linear model that only consists of an intercept, $\beta_0$.
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the basic model with every predictor, we find that the [Square Feet]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.
```{r}
library(ggplot2)
library(data.table)
library(dplyr)
library(stringr)
library(car)
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo",
                    "pets_allowed", "square_feet",
                    "population", "density")
new <- regression(regressor_list)
table <- data.frame(
  Statistic = c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS"), Values = new)
knitr::kable(table)
```
:::
:::
:::

## Forward Stepwise Search {auto-animate=true}

### Second Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the second iteration of our stepwise search, we use the linear model from the previous iteration, being the model with an intercept $\beta_0$ and a single predictor [Square Feet]{.bg style="--col: #D0E1F9"}, denoted as $X_1$.
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0 + X_1\beta_1
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the model with every predictor, we find that the [Density]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.

![](second_iteration.png)

:::
:::
:::

## Forward Stepwise Search {auto-animate=true .smaller}

### Second Iteration

::: {.columns}
::: {.column width="50%"}
**Current:**\
![](second_iteration.png){ width=80% }
:::
::: {.column width="50%"}
::: {.fragment .fade-left}
**Previous:**\
![](first_iteration.png){ width=80% }
:::
:::
:::

::: {.fragment}
Comparing the two, we see that the $R^2$ increased significantly, showing that the model's accuracy improved when including [density]{.bg style="--col: #D0E1F9"}. Secondly, **AIC**, **AICc**, **BIC**, and **PRESS** all got smaller, meaning that the model is better even when accounting for complexity and overfitting, and generalizes new data better.
:::

## Forward Stepwise Search {auto-animate=true}

### Third Iteration

::: {.columns}
::: {.column width="50%"}
::: {.fragment}
For the third iteration, we now include both predictors: [Square Feet]{.bg style="--col: #D0E1F9"} (denoted by $X_1$), and [density]{.bg style="--col: #D0E1F9"} (denoted by $X_2$).
:::
::: {.fragment}
::: {.r-stack}
**Model:**
:::
$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2
$$
:::
:::
::: {.column width="50%"}

::: {.fragment}
After fitting the model with every predictor, we find that the [Population]{.bg style="--col: #D0E1F9"} variable results in the lowest P value of the model.

![](third_iteration.png)

:::
:::
:::

## Forward Stepwise Search {auto-animate=true .smaller}

### Third Iteration

::: {.columns}
::: {.column width="50%"}
**Current:**\
![](third_iteration.png){ width=80% }
:::
::: {.column width="50%"}
::: {.fragment .fade-left}
**Previous:**\
![](second_iteration.png){ width=80% }
:::
:::
:::

::: {.fragment}
Yet, when we compare this with the previous iteration we noticed that $R^2$ only increased .... FIX
:::

## Forward Stepwise Search

### Completed

::: {.fragment}
Hence, we have found our linear model, with the predictor variables being [Square Feet]{.bg style="--col: #D0E1F9"} ($X_1$), and [density]{.bg style="--col: #D0E1F9"} ($X_2$).

$$
Y=\beta_0 + X_1\beta_1 + X_2\beta_2
$$
:::
::: {.fragment}
::: {.columns}
::: {.column width="40%"}
::: {.center}
Please put analysis of model here. Doesn't have to be long, just explain $R^2$ and P value.
:::
:::
:::{.column width="60%"}
```{r}
cleaned$price1 <- log(cleaned$price)
model <- lm(price ~ square_feet + density, data=cleaned)
summary_model <- summary(model)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```

:::
:::
:::

## Residuals

### Residuals vs. Fitted Values

```{r}
model <- lm(price ~ square_feet + density, data=cleaned)
model1 <- lm(price1 ~ square_feet + density, data=cleaned)
summary(model)
summary(model1)
plot(model, which = 1)
```
The above plot shows some heteroscedasticity on the higher end of the fitted values, but it's not that much so it's okay. (Shows that model is less accurate for higher fitted values). Listings on either extreme end have some other factors? FIX

## Residuals

### Quantile-Quantile

```{r}
plot(model, which = 2)
plot(model1, which = 2)
```

The Q-Q residual plot shows deviations at both extremes, indicating that our dataset is not normally distributed. This suggests that factors beyond our model may influence prices for listings that are either very inexpensive or highly expensive. For pricier listings, this may be attributable to luxury features, while for more affordable ones, underlying issues with the property could be to blame. (Add to different file and read off of there, this is too long to be on the slide) Use boxCox() to show that taking the log makes for a better model. Compare different models and show that log(price) model is better, better residuals. Maybe do anotehr residual analysis.

## Residuals

### Scale-Location

```{r}
plot(model, which = 3)
plot(model1, which = 3)
```
Distributed normally around the red line, which is good. It starts to go up for higher fitted values, indicating the same thing that the residuals vs. fitted values did (less accurate for higher values).

## Residuals

### Residuals vs. Leverage

```{r}
plot(model, which = 5)
plot(model1, which = 5)
```
There are some high leverage points that fall outside of the Cook distance, two of which we have seen before. These are points worth investigating. FIX

## Adjusted Model

### Mitigate the effects of high leverage points

Please remove oultlier points found in the previous plots (the numbered ones), and show new $R^2$. Points of interest are 20556, 18779, 6754, as they appear in multiple of the plots. Analyze and explain why they are bad.

## Confidence Interval

### Find an actual listing around rice and apply model

```{r}
predictions <- predict(model, data.frame(square_feet = c(2171), density = c(7000)), interval = "confidence", level = 0.95)
predictions

predictions <- predict(model1, data.frame(square_feet = c(2171), density = c(7000)), interval = "confidence", level = 0.95)
exp(predictions)
```
Put into knitr kable, add listing image, explain how we chose density metric. I have no idea how it works. Lowkey just choose one that gives a value that is in the CI. It can fall outside of it i guess, just talk about low r squared and how it doesn't fully account for all of the variance.

## Summary
