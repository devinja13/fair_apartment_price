---
title: "Determining a Fair Apartment Price"
subtitle: "Stat 410 Final Project"
author: "Jack Dzialo, Devin Abraham, Tom Vincent"
format: pdf
---

```{r setup, include=FALSE}
library(ggplot2)
library(data.table)
library(dplyr)
library(stringr)
library(car)
library(knitr)
library(gridExtra)
```

```{r, echo=FALSE, include=FALSE}
data <- read.csv2("apartments.csv")
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
cleaned <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(cleaned, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
```

## Purpose

When coming up with ideas for our final project, we wanted to work on something that related to a real problem that we had in our lives. Our whole group is looking for housing next year, both near Rice and out of Houston. Two of us are looking for larger houses to share with roommates, while one is looking for a one 1-2 bedroom apartment in a different city. Since we are all facing the same problem, but are looking at many different housing factors we wanted to come up with a model to ensure we are getting a fair price for the given housing option, or even find below-market opportunities allowing us to get a great deal. Since there are so many houses on the market all with different qualities, it is difficult to determine a fair price. 

Therefore, for our final project, we aim to address the problem of accurately predicting apartment prices based on a variety of influencing factors. Our primary goal is to develop a reliable regression model that can assess the fairness of rental prices, providing a valuable tool for prospective renters to make informed decisions. The effects of variables such as location, size, amenities, and neighborhood characteristics on rental costs are of particular interest, as understanding these can aid in identifying key drivers of price variations. By achieving these objectives, we hope to contribute to a more transparent rental market.


## Definitions

1.  $AIC$: Measures trade off between model fit and complexity with a penalty for more complexity
2.  $AICc$: Corrected version of AIC with a larger penalty for regressors
3.  $BIC$: Penalizes model complexity heavily and favors simple model with large sample sizes
4.  $PRESS$: Predicted Residual Error Sum of Squares; predicts how accurate a model is at predicting data it hasn't seen before

## Data Collection
We got our data from the UC Irvine Machine Learning repository titled Apartment for Rent. In addition, we supplemented this dataset with a US cities dataset from Simple Maps so we could include the population and density of the city the apartment is located in. The Apartment for Rent dataset included 22 variables and 52,746 oberservations. The 22 variables included id, category, title, body, amenities, bathrooms, bedrooms, currency, fee, has_photo, pets_allowed, price, price_display, price_type, square_feet, address, cityname, state, latitude, longitude, source, time. After exploring the data, we found columns with just text such as Title, Body, and Address which we were not able to turn into quantitative data or factors so we decided to not include it in the model. In addition, we did not include the following columns since they were not directly related to the housing. We removed id, category, currency, price_type, state, latitude, longitude, and time. To include the population and density of the city, were able to merge the datasets including the population and density for the listed city in each row. There were a range of responses for pets_allowed, has_fee, and has_photo but we were able to convert them into binary indicator variables 1 for yes, 0 for no. 


```{r, fig.width=8, fig.height=6, echo=FALSE}

data$price <- as.numeric(data$price)
hist(data$price,
     freq = FALSE,
     breaks = "scott",
     main = "Histogram of Price",
     xlab = "Price (dollars)",
     col = "lightblue",
     xlim = c(0, 10000))
```

##  Data Synopsis for All Variables
```{r data, echo=FALSE}
table1 <- data.frame(
  Statistic = c("Mean Price", "Max Price", "Min Price", "Median Price"),
  Value = c(round(mean(cleaned$price, na.rm = TRUE)),
            round(max(cleaned$price, na.rm = TRUE)),
            round(min(cleaned$price, na.rm = TRUE)),
            round(median(cleaned$price, na.rm = TRUE))
))

data$square_feet <- as.numeric(data$square_feet)

table2 <- data.frame(
  Statistic = c("Mean Square Feet", "Max Square Feet", "Min Square Feet", "Median Square Feet"),
  Value = c(round(mean(cleaned$square_feet, na.rm = TRUE)),
            round(max(cleaned$square_feet, na.rm = TRUE)),
            round(min(cleaned$square_feet, na.rm = TRUE)),
            round(median(cleaned$square_feet, na.rm = TRUE))
))

cleaned$bedrooms <- as.numeric(cleaned$bedrooms)

table3 <- data.frame(
  Statistic = c("Mean Bedrooms", "Max Bedrooms", "Min Bedrooms", "Median Bedrooms"),
  Value = c(round(mean(cleaned$bedrooms, na.rm=TRUE)),
            max(cleaned$bedrooms, na.rm = TRUE),
            min(cleaned$bedrooms, na.rm = TRUE),
            median(cleaned$bedrooms, na.rm=TRUE)
))

cleaned$bathrooms <- as.numeric(cleaned$bathrooms)


table4 <- data.frame(
  Statistic = c("Mean Bathrooms", "Max Bathrooms", "Min Bathrooms", "Median Bathrooms"),
  Value = c(round(mean(cleaned$bathrooms, na.rm=TRUE)),
            max(cleaned$bathrooms, na.rm = TRUE),
            min(cleaned$bathrooms, na.rm = TRUE),
            median(cleaned$bathrooms, na.rm=TRUE)
            
))

table5 <- data.frame(
  Statistic = c("Mean Population", "Max Population", "Min Population", "Median Population"),
  Value = c(round(mean(cleaned$population, na.rm=TRUE)),
            max(cleaned$population, na.rm = TRUE),
            min(cleaned$population, na.rm = TRUE),
            median(cleaned$population, na.rm=TRUE)
            
))

table6 <- data.frame(
  Statistic = c("Mean Density", "Max Density", "Min Density", "Median Density"),
  Value = c(round(mean(cleaned$density, na.rm=TRUE)),
            max(cleaned$density, na.rm = TRUE),
            min(cleaned$density, na.rm = TRUE),
            median(cleaned$density, na.rm=TRUE)
            
))

table7 <- data.frame(
  Statistic = c("Mean Density", "Max Density", "Min Density", "Median Density"),
  Value = c(round(mean(cleaned$density, na.rm=TRUE)),
            max(cleaned$density, na.rm = TRUE),
            min(cleaned$density, na.rm = TRUE),
            median(cleaned$density, na.rm=TRUE)
            
))

table8 <- data.frame(
  Statistic = c("Proportion with Photos", "Proportion with fees", "Proportion that allow pets"),
  Value = c(round(sum(cleaned$has_photo)/nrow(cleaned), 3),
            round(sum(cleaned$fee)/nrow(cleaned), 3),
            round(sum(cleaned$pets_allowed)/nrow(cleaned), 3)
))

# Convert tables to ggplot objects
table1_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table1))
table2_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table2))

# Charts display side by side
grid.arrange(table1_ggplot, table2_ggplot, ncol = 2)

table3_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table3))
table4_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table4))
grid.arrange(table3_ggplot, table4_ggplot, ncol = 2)

table5_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table5))
table6_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table6))
grid.arrange(table5_ggplot, table6_ggplot, ncol = 2)

table7_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table7))
table8_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table8))
grid.arrange(table7_ggplot, table8_ggplot, ncol = 2)
```


# Data Analysis - Model Building

## Forward Stepwise Search

### First Iteration

For the first iteration of our stepwise search, we start off with a linear model that only consists of an intercept, $\beta_0$.

$$
Y = \beta_0
$$

After fitting the basic model with every predictor, we find that the Square feet variable results in the lowest P value of the model. We created an R function to
run forward stepwise search that inputs the potential variables to be included and
the variables already in the model.

```{r, echo=FALSE}

regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + (2*npar*(npar + 1) / (n - npar - 1))
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, round(low_pval, 4), round(adj_r2, 4), round(AIC), round(AICc), round(BIC), round(press_statistic)))
}
regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo",
                    "pets_allowed", "square_feet",
                    "population", "density")
new <- regression(regressor_list)
table <- data.frame(
  Statistic = c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS"), Values = new)
knitr::kable(table)
```

## Forward Stepwise Search {auto-animate="true"}

### Second Iteration
For the second iteration of our stepwise search, we use the linear model from the previous iteration, being the model with an intercept $\beta_0$ and a single predictor square feet, denoted as $X_1$.


$$
Y = \beta_0 + X_1\beta_1
$$

After fitting the model with every predictor, we find that the Density variable results in the lowest P value of the model.
```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo","pets_allowed",
                    "population", "density")
new1 <- regression(regressor_list, "square_feet")

new_sub <- c(NA, NA, as.numeric(new[3:7]))
new1_sub <- c(NA, NA, as.numeric(new1[3:7]))
diff <- round(c(new_sub - new1_sub), 3)

mat <- matrix(data=c(new, new1, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)
```


## Forward Stepwise Search

### Second Iteration

The model improved when including density, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo", "pets_allowed", "population")
new2 <- regression(regressor_list, c("square_feet", "density"))
new1_sub <- c(NA, NA, as.numeric(new1[3:7]))
new2_sub <- c(NA, NA, as.numeric(new2[3:7]))
diff <- round(c(new1_sub - new2_sub), 3)

mat <- matrix(data=c(new1, new2, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Third Iteration

The model improved when including population, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo", "pets_allowed")
new3 <- regression(regressor_list, c("square_feet", "density", "population"))
new2_sub <- c(NA, NA, as.numeric(new2[3:7]))
new3_sub <- c(NA, NA, as.numeric(new3[3:7]))
diff <- round(c(new2_sub - new3_sub), 3)

mat <- matrix(data=c(new2, new3, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Fourth Iteration

The model improved when including bathrooms, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "fee", "has_photo", "pets_allowed")
new4 <- regression(regressor_list, c("bathrooms","square_feet", "density", "population"))
new3_sub <- c(NA, NA, as.numeric(new3[3:7]))
new4_sub <- c(NA, NA, as.numeric(new4[3:7]))
diff <- round(c(new3_sub - new4_sub), 3)

mat <- matrix(data=c(new3, new4, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```
### Fifth Iteration

The model improved when including bedrooms, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5
$$

```{r, echo=FALSE}

regressor_list <- c( "fee", "has_photo", "pets_allowed")
new5 <- regression(regressor_list, c("bedrooms","bathrooms","square_feet", "density", "population"))
new4_sub <- c(NA, NA, as.numeric(new4[3:7]))
new5_sub <- c(NA, NA, as.numeric(new5[3:7]))
diff <- round(c(new4_sub - new5_sub), 3)

mat <- matrix(data=c(new4, new5, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Sixth Iteration

The model improved when including has_photo, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5 + X_6\beta_6
$$

```{r, echo=FALSE}

regressor_list <- c( "fee", "pets_allowed")
new6 <- regression(regressor_list, c("has_photo","bedrooms","bathrooms","square_feet", "density", "population"))
new5_sub <- c(NA, NA, as.numeric(new5[3:7]))
new6_sub <- c(NA, NA, as.numeric(new6[3:7]))
diff <- round(c(new5_sub - new6_sub), 3)

mat <- matrix(data=c(new5, new6, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("Sixth Iteration", "Seveneth Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Seventh Iteration

The model improved when including pets_allowed, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5 + X_6\beta_6 + X_7\beta_7
$$

```{r, echo=FALSE}

regressor_list <- c("fee")
new7 <- regression(regressor_list, c("has_photo","bedrooms","bathrooms","square_feet", "density", "population", "pets_allowed"))
new6_sub <- c(NA, NA, as.numeric(new6[3:7]))
new7_sub <- c(NA, NA, as.numeric(new7[3:7]))
diff <- round(c(new6_sub - new7_sub), 3)

mat <- matrix(data=c(new6, new7, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("Seventh Iteration", "Eight Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```
## Forward Stepwise Search

### Completed


Hence, we have found our linear model since the previous iteration did not improve the model, with the predictor variables being Square feet ($X_1$), and denstiy ($X_2$).

$$
Y=\beta_0 + X_1\beta_1 + X_2\beta_2
$$


After fitting the model with every predictor, we find that the Population variable results in the lowest P value of the model.



$32\%$ of the variation in price explained by model


```{r, echo=FALSE}

library(car)
cleaned$price1 <- log(cleaned$price)

# Update the model to include all specified regressors
model <- lm(price ~ square_feet + density + log(population) + bathrooms + bedrooms + has_photo + pets_allowed, data=cleaned)

# Summarize the model
summary_model <- summary(model)

# Update the table to include all regressors
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density", "Population", "Bathrooms", "Bedrooms", "Has Photo", "Pets Allowed"),
  Values = c(
    round(summary_model$coefficients[1, 1], 4), 
    round(summary_model$coefficients[2, 1], 4), 
    round(summary_model$coefficients[3, 1], 4), 
    round(summary_model$coefficients[4, 1], 4), 
    round(summary_model$coefficients[5, 1], 4), 
    round(summary_model$coefficients[6, 1], 4), 
    round(summary_model$coefficients[7, 1], 4), 
    round(summary_model$coefficients[8, 1], 4)
  ),
  SE = c(
    round(summary_model$coefficients[1, 2], 4), 
    round(summary_model$coefficients[2, 2], 4), 
    round(summary_model$coefficients[3, 2], 4), 
    round(summary_model$coefficients[4, 2], 4), 
    round(summary_model$coefficients[5, 2], 4), 
    round(summary_model$coefficients[6, 2], 4), 
    round(summary_model$coefficients[7, 2], 4), 
    round(summary_model$coefficients[8, 2], 4)
  ),
  P_Value = c(
    summary_model$coefficients[1, 4], 
    summary_model$coefficients[2, 4], 
    summary_model$coefficients[3, 4], 
    summary_model$coefficients[4, 4], 
    summary_model$coefficients[5, 4], 
    summary_model$coefficients[6, 4], 
    summary_model$coefficients[7, 4], 
    summary_model$coefficients[8, 4]
  ),
  R_Squared = c(round(summary_model$r.squared, 4), "", "", "", "", "", "", "")
)

# Create the table
knitr::kable(table, caption = "Summary of Model")

```


# Analysis of Residuals - Model Improvement

## Residuals

### Residuals vs. Fitted Values

```{r, echo=FALSE}
model <- lm(price ~ square_feet + density + log(population) + bathrooms + bedrooms + has_photo + pets_allowed, data=cleaned)
plot(model, which = 1)
```

We can see slight heteroskedasticity in this plot with a slight downward trend. 
As the fitted values become greater the residuals are greater indicating some model inaccuracy in high ranges. 

## Residuals

### Quantile-Quantile

```{r, echo=FALSE}
plot(model, which = 2)
```

Relatively normal, room for improvement

## Residuals

### Scale-Location

```{r, echo=FALSE}
plot(model, which = 3)
```

The Scale Location plot is not ideal, as the red line should be horizontal. The means homoscedasticity is violated, but this is likely to have higher variation in high-fitted values. As prices become more expensive, there are factors we can not account for, especially with luxury apartments.

## Residuals

### Residuals vs. Leverage

```{r, echo=FALSE}
plot(model, which = 5)
```

The dotted lines represent Cook's distance, highlighting several points that surpass its threshold. These points indicate high-leverage observations, which warrant further analysis to determine if they are outliers and whether they accurately reflect the relationship in question.

## Adjusted Model

### Outliers and Data Transformation

Address non-normality through Box-Cox transformation

```{r, echo=FALSE}
boxCox(model)
```

-   Optimal $\lambda$ close to 0

$$
y(\lambda) =
\begin{cases}
\frac{y^\lambda - 1}{\lambda}, & \text{if } \lambda \neq 0 \\[10pt]
\ln(y), & \text{if } \lambda = 0
\end{cases}
$$

## Adjusted Model

### Summary

Our adjusted model is: $\text{ln}(Y)=\beta_0+X_1\beta_1+X_2\beta_2$

```{r, echo=FALSE}
cleaned_new <- cleaned[-c(6754, 20556, 18779, 29745), ]
cleaned_new1 <- cleaned_new[-15668, ]
model_new <- lm(log(price) ~ square_feet + density, data=cleaned_new1)
summary_model <- summary(model_new)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```


## Adjusted Model Residuals

### Residuals vs. Fitted Values

**Previous**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model, which = 1)
```

**Adjusted**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model_new, which = 1)
```

Normallity improved slightly

## Adjusted Model Residuals

### Quantile-Quantile

**Previous**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model, which = 2)
```
**Adjusted**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model_new, which = 2)
```

Significant Improvment 


## Adjusted Model Residuals

### Scale-Location


**Previous**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model, which = 3)
```

**Adjusted**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model_new, which = 3)
```

Slight homoscedasticity

## Adjusted Model Residuals

### Residuals vs. Leverage

**Previous**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model, which = 5)
```

**Adjusted**

```{r fig.height=8, fig.width=10, echo=FALSE}
plot(model_new, which = 5)
```

Less high leverage points/outliers

## Confidence Interval

### Real Apartment Listing

The confidence interval generated by the model:

```{r, echo=FALSE}
calculated_density <- cities %>% filter(city == "houston", state_id == "TX") %>% select(density)
predictions <- predict(model_new, data.frame(square_feet = c(1200), density = c(2*calculated_density)), interval = "confidence", level = 0.95)
knitr::kable(exp(predictions))
```


-   Density metric was taken from the dataset
-   Scaled to account for higher density


Good Prediction

## Summary


-   Explored the data and found 8 usable regressors 5 of which were continuous and 3 of which were categorical
-   We used Forward Step wise Search to build our model and found 2 regressors for our final model, square_feet and density
-   Adjusted our model to take the log of price which improved $R^2$

Limitations


-   Since Forward Step Wise Search is a greedy algorithm so it makes the best choice at each step and does not guarentee the most optimal solution
-   In the future we would like a data set with more regressors, our group hypothesizes that a regressor that represents the apartments luxuriousness would have a significant impact
    -   Could also use interaction terms to account for nonlinear effects
    
    
## Reflection

Our group spent about 18 hours on this project. Our first problem was that we couldn’t find good data. Initially, we started with the idea to predict the probability that a NCAA Baseball Team wins a game given a log of all NCAA college games from last year. But, we ran into the problem with the data that we had hoped to use. Luckily, a few of our group members brought up that they were looking for apartments next year and so we decided to find data that fit that idea. We were able to find clean data and started our forward selection process to find the most important variables. Our second problem was that we had a few outlier points. Instead of removing without any evidence as to if it was a high leverage point, initially we removed these points. After further thought, we decided to test if these points were of high leverage before removing and gave us more insight into our data. 

The following were some items that we learned by doing this project: Forward Stepwise Search, and specifically how to decide whether to add a variable into the data or NOT. AIC and its variants, which helps measure trade off between model fit and complexity with a penalty for more complexity introduced into the model. How to look for GOOD data, i.e. data that is easy to clean and friendly in R’s environment. How to read different plots, such as the Residuals vs. Fitted Values, the Quantile Quantile Plot, and the Residuals vs. Leverage. For instance, for the Quantile Quantile plot, we were able to determine if our data compares to the distribution of a normal distribution. How to adjust the model such as adding a scaling function like a natural logarithm function. 

Advice that I would give students next time and that I wish I knew was how important it is to pick good data. This made our process a lot easier because the data was easily manipulated in R.

## Sources

Apartment for Rent Classified [Dataset]. (2019). UCI Machine Learning Repository. https://doi.org/10.24432/C5X623.

## Appendix

## Histogram

```{r fig.width=8, fig.height=6 }
data <- read.csv2("apartments.csv")
data$price <- as.numeric(data$price)
```


## Function to determine which variable has the lowest P-Value

```{r}
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
```




--------------------------------------------------------------------
