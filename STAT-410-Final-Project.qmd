---
title: "Determining a Fair Apartment Price"
subtitle: "Stat 410 Final Project"
author: "Jack Dzialo, Devin Abraham, Tom Vincent"
format: pdf
---

```{r setup, include=FALSE}
library(ggplot2)
library(data.table)
library(dplyr)
library(stringr)
library(car)
library(knitr)
library(gridExtra)
library(gridGraphics)
library(ggfortify)
```

```{r, echo=FALSE, include=FALSE}
data <- read.csv2("apartments.csv")
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
cleaned <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(cleaned, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
data$price <- as.numeric(data$price)
data$square_feet <- as.numeric(data$square_feet)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$population = cleaned$population / 1000
```

## Purpose

When coming up with ideas for our final project, we wanted to work on something that related to a real problem that we had in our lives. Our whole group is looking for housing next year, both near Rice and out of Houston. Two of us are looking for larger houses to share with roommates, while one is looking for a one 1-2 bedroom apartment in a different city. Since we are all facing the same problem, but are looking at many different housing factors we wanted to come up with a model to ensure we are getting a fair price for the given housing option, or even find below-market opportunities allowing us to get a great deal. Since there are so many houses on the market all with different qualities, it is difficult to determine a fair price.

Therefore, for our final project, we aim to address the problem of accurately predicting apartment prices based on a variety of influencing factors. Our primary goal is to develop a reliable regression model that can assess the fairness of rental prices, providing a valuable tool for prospective renters to make informed decisions. The effects of variables such as location, size, amenities, and neighborhood characteristics on rental costs are of particular interest, as understanding these can aid in identifying key drivers of price variations. By achieving these objectives, we hope to contribute to a more transparent rental market.


## Definitions

1.  $AIC$: Measures trade off between model fit and complexity with a penalty for more complexity
2.  $AICc$: Corrected version of AIC with a larger penalty for regressors
3.  $BIC$: Penalizes model complexity heavily and favors simple model with large sample sizes
4.  $PRESS$: Predicted Residual Error Sum of Squares; predicts how accurate a model is at predicting data it hasn't seen before

## Data Collection
We got our data from the UC Irvine Machine Learning repository titled Apartment for Rent. In addition, we supplemented this dataset with a US cities dataset from Simple Maps so we could include the population and density of the city the apartment is located in. The Apartment for Rent dataset included 22 variables and 52,746 oberservations. The 22 variables included id, category, title, body, amenities, bathrooms, bedrooms, currency, fee, has_photo, pets_allowed, price, price_display, price_type, square_feet, address, cityname, state, latitude, longitude, source, time. After exploring the data, we found columns with just text such as Title, Body, and Address which we were not able to turn into quantitative data or factors so we decided to not include it in the model. In addition, we did not include the following columns since they were not directly related to the housing. We removed id, category, currency, price_type, state, latitude, longitude, and time. To include the population and density of the city, were able to merge the datasets including the population and density for the listed city in each row. There were a range of responses for pets_allowed, has_fee, and has_photo but we were able to convert them into binary indicator variables 1 for yes, 0 for no.


```{r, fig.width=8, fig.height=6, echo=FALSE}

hist(data$price,
     freq = FALSE,
     breaks = "scott",
     main = "Histogram of Price",
     xlab = "Price (dollars)",
     col = "lightblue",
     xlim = c(0, 10000))
```

##  Data Synopsis for All Variables
```{r data, echo=FALSE}
table1 <- data.frame(
  Statistic = c("Mean Price", "Max Price", "Min Price", "Median Price"),
  Value = c(round(mean(cleaned$price, na.rm = TRUE)),
            round(max(cleaned$price, na.rm = TRUE)),
            round(min(cleaned$price, na.rm = TRUE)),
            round(median(cleaned$price, na.rm = TRUE))
))


table2 <- data.frame(
  Statistic = c("Mean Square Feet", "Max Square Feet", "Min Square Feet", "Median Square Feet"),
  Value = c(round(mean(cleaned$square_feet, na.rm = TRUE)),
            round(max(cleaned$square_feet, na.rm = TRUE)),
            round(min(cleaned$square_feet, na.rm = TRUE)),
            round(median(cleaned$square_feet, na.rm = TRUE))
))


table3 <- data.frame(
  Statistic = c("Mean Bedrooms", "Max Bedrooms", "Min Bedrooms", "Median Bedrooms"),
  Value = c(round(mean(cleaned$bedrooms, na.rm=TRUE)),
            max(cleaned$bedrooms, na.rm = TRUE),
            min(cleaned$bedrooms, na.rm = TRUE),
            median(cleaned$bedrooms, na.rm=TRUE)
))



table4 <- data.frame(
  Statistic = c("Mean Bathrooms", "Max Bathrooms", "Min Bathrooms", "Median Bathrooms"),
  Value = c(round(mean(cleaned$bathrooms, na.rm=TRUE)),
            max(cleaned$bathrooms, na.rm = TRUE),
            min(cleaned$bathrooms, na.rm = TRUE),
            median(cleaned$bathrooms, na.rm=TRUE)

))

table5 <- data.frame(
  Statistic = c("Mean Population", "Max Population", "Min Population", "Median Population"),
  Value = c(round(mean(cleaned$population, na.rm=TRUE)),
            max(cleaned$population, na.rm = TRUE),
            min(cleaned$population, na.rm = TRUE),
            median(cleaned$population, na.rm=TRUE)

))

table6 <- data.frame(
  Statistic = c("Mean Density", "Max Density", "Min Density", "Median Density"),
  Value = c(round(mean(cleaned$density, na.rm=TRUE)),
            max(cleaned$density, na.rm = TRUE),
            min(cleaned$density, na.rm = TRUE),
            median(cleaned$density, na.rm=TRUE)

))

table7 <- data.frame(
  Statistic = c("Mean Density", "Max Density", "Min Density", "Median Density"),
  Value = c(round(mean(cleaned$density, na.rm=TRUE)),
            max(cleaned$density, na.rm = TRUE),
            min(cleaned$density, na.rm = TRUE),
            median(cleaned$density, na.rm=TRUE)

))

table8 <- data.frame(
  Statistic = c("Proportion with Photos", "Proportion with fees", "Proportion that allow pets"),
  Value = c(round(sum(cleaned$has_photo)/nrow(cleaned), 3),
            round(sum(cleaned$fee)/nrow(cleaned), 3),
            round(sum(cleaned$pets_allowed)/nrow(cleaned), 3)
))

# Convert tables to ggplot objects
table1_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table1))
table2_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table2))

# Charts display side by side
grid.arrange(table1_ggplot, table2_ggplot, ncol = 2)

table3_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table3))
table4_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table4))
grid.arrange(table3_ggplot, table4_ggplot, ncol = 2)

table5_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table5))
table6_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table6))
grid.arrange(table5_ggplot, table6_ggplot, ncol = 2)

table7_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table7))
table8_ggplot <- ggplot(data = NULL) + geom_blank() + theme_minimal() + annotation_custom(tableGrob(table8))
grid.arrange(table7_ggplot, table8_ggplot, ncol = 2)
```


# Data Analysis - Model Building

## Forward Stepwise Search

### First Iteration

For the first iteration of our stepwise search, we start off with a linear model that only consists of an intercept, $\beta_0$.

$$
Y = \beta_0
$$

After fitting the basic model with every predictor, we find that the Square feet variable results in the lowest P value of the model. We created an R function to
run forward stepwise search that inputs the potential variables to be included and
the variables already in the model.

```{r, echo=FALSE}
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + (2*npar*(npar + 1) / (n - npar - 1))
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, round(low_pval, 4), round(adj_r2, 4), round(AIC), round(AICc), round(BIC), round(press_statistic)))
}
regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo",
                    "pets_allowed", "square_feet",
                    "population", "density")
new <- regression(regressor_list)
table <- data.frame(
  Statistic = c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS"), Values = new)
knitr::kable(table)
```

## Forward Stepwise Search {auto-animate="true"}

### Second Iteration
For the second iteration of our stepwise search, we use the linear model from the previous iteration, being the model with an intercept $\beta_0$ and a single predictor square feet, denoted as $X_1$.


$$
Y = \beta_0 + X_1\beta_1
$$

After fitting the model with every predictor, we find that the Density variable results in the lowest P value of the model.
```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo","pets_allowed",
                    "population", "density")
new1 <- regression(regressor_list, "square_feet")

new_sub <- c(NA, NA, as.numeric(new[3:7]))
new1_sub <- c(NA, NA, as.numeric(new1[3:7]))
diff <- round(c(new_sub - new1_sub), 3)

mat <- matrix(data=c(new, new1, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)
```


## Forward Stepwise Search

### Second Iteration

The model improved when including density, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo", "pets_allowed", "population")
new2 <- regression(regressor_list, c("square_feet", "density"))
new1_sub <- c(NA, NA, as.numeric(new1[3:7]))
new2_sub <- c(NA, NA, as.numeric(new2[3:7]))
diff <- round(c(new1_sub - new2_sub), 3)

mat <- matrix(data=c(new1, new2, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Third Iteration

The model improved when including population, while $AIC, AICc, BIC, PRESS$ got smaller. We also decided to divide population by 1000, as the values of population are very high, leading to a very small $\beta$ value. Dividing by 1000 leads to a higher $\beta$ estimate, which is easier to understand, as it does not get truncated by functions that display the coefficients and their corresponding important information.

$Model:$


$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo", "pets_allowed")
new3 <- regression(regressor_list, c("square_feet", "density", "population"))
new2_sub <- c(NA, NA, as.numeric(new2[3:7]))
new3_sub <- c(NA, NA, as.numeric(new3[3:7]))
diff <- round(c(new2_sub - new3_sub), 3)

mat <- matrix(data=c(new2, new3, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Fourth Iteration

The model improved when including bathrooms, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4
$$

```{r, echo=FALSE}

regressor_list <- c("bedrooms", "fee", "has_photo", "pets_allowed")
new4 <- regression(regressor_list, c("bathrooms","square_feet", "density", "population"))
new3_sub <- c(NA, NA, as.numeric(new3[3:7]))
new4_sub <- c(NA, NA, as.numeric(new4[3:7]))
diff <- round(c(new3_sub - new4_sub), 3)

mat <- matrix(data=c(new3, new4, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```
### Fifth Iteration

The model improved when including bedrooms, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5
$$

```{r, echo=FALSE}

regressor_list <- c( "fee", "has_photo", "pets_allowed")
new5 <- regression(regressor_list, c("bedrooms","bathrooms","square_feet", "density", "population"))
new4_sub <- c(NA, NA, as.numeric(new4[3:7]))
new5_sub <- c(NA, NA, as.numeric(new5[3:7]))
diff <- round(c(new4_sub - new5_sub), 3)

mat <- matrix(data=c(new4, new5, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("First Iteration", "Second Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Sixth Iteration

The model improved when including has_photo, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5 + X_6\beta_6
$$

```{r, echo=FALSE}

regressor_list <- c( "fee", "pets_allowed")
new6 <- regression(regressor_list, c("has_photo","bedrooms","bathrooms","square_feet", "density", "population"))
new5_sub <- c(NA, NA, as.numeric(new5[3:7]))
new6_sub <- c(NA, NA, as.numeric(new6[3:7]))
diff <- round(c(new5_sub - new6_sub), 3)

mat <- matrix(data=c(new5, new6, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("Sixth Iteration", "Seveneth Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```

### Seventh Iteration

The model improved when including pets_allowed, while $AIC, AICc, BIC, PRESS$ got smaller.

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5 + X_6\beta_6 + X_7\beta_7
$$

```{r, echo=FALSE}

regressor_list <- c("fee")
new7 <- regression(regressor_list, c("has_photo","bedrooms","bathrooms","square_feet", "density", "population", "pets_allowed"))
new6_sub <- c(NA, NA, as.numeric(new6[3:7]))
new7_sub <- c(NA, NA, as.numeric(new7[3:7]))
diff <- round(c(new6_sub - new7_sub), 3)

mat <- matrix(data=c(new6, new7, diff), nrow=3, byrow=TRUE, )

colnames(mat) <- c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS")


rownames(mat) <- c("Seventh Iteration", "Eight Iteration", "Difference")

table <- as.data.frame(mat)

knitr::kable(table)

```
## Forward Stepwise Search

### Completed


Hence, we have found our linear model since the previous iteration did not improve the model, with the predictor variables being Square feet ($X_1$), density ($X_2$), population($X_3$), bathrooms ($X_4$), bedrooms ($X_5$), has photo ($X_6$), and pets allowed ($X_7$).

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_5\beta_5 + X_6\beta_6 + X_7\beta_7
$$


After fitting the model with every predictor, we find that the Population variable results in the lowest P value of the model.

```{r, echo=FALSE}

library(car)
cleaned$price1 <- log(cleaned$price)

# Update the model to include all specified regressors
model <- lm(price ~ square_feet + density + population + bathrooms + bedrooms + has_photo + pets_allowed, data=cleaned)

# Summarize the model
summary_model <- summary(model)

# Update the table to include all regressors
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density", "Population", "Bathrooms", "Bedrooms", "Has Photo", "Pets Allowed"),
  Values = c(
    round(summary_model$coefficients[1, 1], 4),
    round(summary_model$coefficients[2, 1], 4),
    round(summary_model$coefficients[3, 1], 4),
    round(summary_model$coefficients[4, 1], 4),
    round(summary_model$coefficients[5, 1], 4),
    round(summary_model$coefficients[6, 1], 4),
    round(summary_model$coefficients[7, 1], 4),
    round(summary_model$coefficients[8, 1], 4)
  ),
  SE = c(
    round(summary_model$coefficients[1, 2], 4),
    round(summary_model$coefficients[2, 2], 4),
    round(summary_model$coefficients[3, 2], 4),
    round(summary_model$coefficients[4, 2], 4),
    round(summary_model$coefficients[5, 2], 4),
    round(summary_model$coefficients[6, 2], 4),
    round(summary_model$coefficients[7, 2], 4),
    round(summary_model$coefficients[8, 2], 4)
  ),
  P_Value = c(
    summary_model$coefficients[1, 4],
    summary_model$coefficients[2, 4],
    summary_model$coefficients[3, 4],
    summary_model$coefficients[4, 4],
    summary_model$coefficients[5, 4],
    summary_model$coefficients[6, 4],
    summary_model$coefficients[7, 4],
    summary_model$coefficients[8, 4]
  ),
  R_Squared = c(round(summary_model$r.squared, 4), "", "", "", "", "", "", "")
)

# Create the table
knitr::kable(table, caption = "Summary of Model")
```
Note that the coefficient for bedrooms is negative, which we thought was suspicious as usually more bedrooms are more valuable when looking for houses. We thought that this might be due to multicollinearity with the bathrooms variable, so we decided to plot the two against each other.

```{r, echo=FALSE, fig.height=3}
gg <- ggplot(cleaned, aes(x = bedrooms, y = bathrooms)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
suppressMessages(print(gg))
```
We noticed that there seems to be a trend, as when amount of bedrooms go up, amount of bathrooms go up. The $R^2$ value of the line of best fit was also 0.46, indicating a clear trend. Therefore, we decided to remove bedrooms from our model, due to its redundancy, given its correlation with the bathrooms predictor. The $R^2$ of the new model decreases slightly, but is acceptable due to it being a reduction of overfitting.

# Analysis of Residuals - Model Improvement

We will now conduct a thorough examination of the model's residuals to identify any potential issues.

### Residuals vs. Fitted Values

```{r, echo=FALSE}
model <- lm(price ~ square_feet + density + population + bathrooms + has_photo + pets_allowed, data=cleaned)
autoplot(model)
```

We can see slight heteroskedasticity in the Residuals vs. Fitted plot, with a slight downward trend.
There are a few outliers points and the model decreases in accuracy for extreme fitted values. Since our model is not able to discern luxury apartments this may explain some of the large outliers. The Q-Q residual plot shows a normal distribution for a good amount of the data. The plot has deviates from the normal distribution around the extremes, meaning our model struggles to predict very large or small rents, but for the vast majority of the data it follows a normal distribution. Again this could be due to features of apartments that our data could not capture, like luxury accommodations or a bad location. The Scale Location plot is not ideal, as the blue line should be horizontal. The means homoscedasticity is violated, but this is likely to have higher variation in high-fitted values. As prices become more expensive, there are factors we can not account for, especially with luxury apartments. The Residuals vs. Leverage plot shows high leverage points, highlighting points that warrant further analysis to determine if they are outliers and whether they accurately reflect the relationship in question.

## Outliers and Data Transformation

A few points show up multiple times as outliers/high leverage points, being the apartments denoted by the indexes 6754, 18779, and 20556. After analyzing these points, we see that their price differs from the price in the description of the apartment, leading us to believe that these apartments were mistakenly inputted into the dataset. To address the non-normality and heteroscedasticity observed in some plots, we applied a Box-Cox transformation to the data. This transformation assists in normalizing the data and stabilizing variance.
```{r, echo=FALSE}
boxCox(model)
```
The BoxCox formula is: $y(\lambda) = \frac{y^\lambda - 1}{\lambda}  \text{ if } \lambda \neq 0, \ln(y), \text{ if } \lambda = 0$ Since the optimal lambda value is close to 0, we can simply take a logarithm of the price, as defined by the BoxCox formula. Our adjusted model is then:
$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2 + X_3\beta_3 + X_4\beta_4 + X_6\beta_6 + X_7\beta_7
$$
where the predictors are square feet ($X_1$), density ($X_2$), population($X_3$), bathrooms ($X_4$), has photo ($X_6$), and pets allowed ($X_7$).
```{r, echo=FALSE}
cleaned_new <- cleaned[-c(6754, 20556, 18779, 29745), ]
cleaned_new1 <- cleaned_new[-15668, ]
cleaned_new2 <- cleaned_new[-c(15668, 3895), ]
cleaned_new3 <- cleaned_new[-c(15668), ]
model_new <- lm(log(price) ~ square_feet + density + log(population) + bathrooms + has_photo + pets_allowed, data=cleaned_new3)
summary_model <- summary(model_new)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density", "Population", "Bathrooms", "Has Photo", "Pets Allowed"),
  Values = c(
    round(summary_model$coefficients[1, 1], 4),
    round(summary_model$coefficients[2, 1], 4),
    round(summary_model$coefficients[3, 1], 4),
    round(summary_model$coefficients[4, 1], 4),
    round(summary_model$coefficients[5, 1], 4),
    round(summary_model$coefficients[6, 1], 4),
    round(summary_model$coefficients[7, 1], 4)
  ),
  SE = c(
    round(summary_model$coefficients[1, 2], 4),
    round(summary_model$coefficients[2, 2], 4),
    round(summary_model$coefficients[3, 2], 4),
    round(summary_model$coefficients[4, 2], 4),
    round(summary_model$coefficients[5, 2], 4),
    round(summary_model$coefficients[6, 2], 4),
    round(summary_model$coefficients[7, 2], 4)
  ),
  P_Value = c(
    summary_model$coefficients[1, 4],
    summary_model$coefficients[2, 4],
    summary_model$coefficients[3, 4],
    summary_model$coefficients[4, 4],
    summary_model$coefficients[5, 4],
    summary_model$coefficients[6, 4],
    summary_model$coefficients[7, 4]
  ),
  R_Squared = c(round(summary_model$r.squared, 4), "", "", "", "", "", "")
)

# Create the table
knitr::kable(table, caption = "Summary of Model")
```

The $R^2$ improved from 0.3433 to 0.3455, which is a very slight but noticeable improvement. Let us now see the effects the transformation had on the residuals, which is more important.
```{r, echo=FALSE}
autoplot(model_new)
```
In the Residuals vs Fitted plot, residuals for lower-priced houses are more evenly distributed around zero, although the model still struggles with higher-priced apartments, as indicated by the skewness of the red line. The QQ plot reflects enhanced accuracy, particularly in the upper tail, with the maximum of the standardized residuals decreasing from nearly 60 to about 5, showing the theoretical and predicted residuals are closer together. The Scale-Location plot suggests that while homoscedasticity issues persist for high fitted values, the data for lower fitted values is now more normally distributed. The Residuals vs Leverage plot shows similar information, but since we removed all high leverage points (which were outliers), there are no points which lie outside of the Cook's distance. Overall, the normality and homoscedasticity of the residuals improved, indicating that our transformation made the linear model more accurate.


## Confidence Interval for a Real Apartment Listing

To test our model on relevant data, we found a real apartment listing around Rice. The density metric was taken from the dataset, multiplied by 2 to account for the higher density of the area around the Med Center.  The confidence interval generated by the model:

::: {.columns}
::: {.column width=70%}

![](./apartment_photo.png){width=400 .center}
:::
::: {.column width=30%}
```{r, echo=FALSE}
calculated_density <- cities %>% filter(city == "houston", state_id == "TX") %>% select(density)
predictions <- predict(model_new, data.frame(square_feet = c(1200), density = c(2*calculated_density), population=c(2300000), bathrooms = c(2),has_photo=c(1), pets_allowed=c(0)), interval = "confidence", level = 0.95)
knitr::kable(exp(predictions))
```
:::
:::

We see that the predicted price falls within the confidence interval! Especially with the high variance of the data, this is a successful result.

## Summary


In our analysis, we explored the data and identified eight usable regressors, comprising five continuous and three categorical variables. We employed a Forward Stepwise Search to construct our model, using 6 of the 8 regressors, then enhanced it by taking the logarithm of the price, which resulted in an improved normality and $R^2$. However, there are some limitations to our approach. Forward Stepwise Search is a greedy algorithm, meaning it makes the best choice at each step but does not guarantee the most optimal solution overall. In future work, we aim to expand our dataset to include more regressors. Our group hypothesizes that a regressor representing the luxuriousness of an apartment could have a significant impact on the model. Additionally, incorporating interaction terms might help account for nonlinear effects.


## Reflection

Our group spent about 18 hours on this project. Our first problem was that we couldn’t find good data. Initially, we started with the idea to predict the probability that a NCAA Baseball Team wins a game given a log of all NCAA college games from last year. But, we ran into the problem with the data that we had hoped to use. Luckily, a few of our group members brought up that they were looking for apartments next year and so we decided to find data that fit that idea. We were able to find clean data and started our forward selection process to find the most important variables. Our second problem was that we had a few outlier points. Instead of removing without any evidence as to if it was a high leverage point, initially we removed these points. After further thought, we decided to test if these points were of high leverage before removing and gave us more insight into our data.

The following were some items that we learned by doing this project: Forward Stepwise Search, and specifically how to decide whether to add a variable into the data or NOT. AIC and its variants, which helps measure trade off between model fit and complexity with a penalty for more complexity introduced into the model. How to look for GOOD data, i.e. data that is easy to clean and friendly in R’s environment. How to read different plots, such as the Residuals vs. Fitted Values, the Quantile Quantile Plot, and the Residuals vs. Leverage. For instance, for the Quantile Quantile plot, we were able to determine if our data compares to the distribution of a normal distribution. How to adjust the model such as adding a scaling function like a natural logarithm function.

Advice that I would give students next time and that I wish I knew was how important it is to pick good data. This made our process a lot easier because the data was easily manipulated in R.

## Sources

Apartment for Rent Classified [Dataset]. (2019). UCI Machine Learning Repository. https://doi.org/10.24432/C5X623.

## Appendix

## Histogram

```{r fig.width=8, fig.height=6 }
data <- read.csv2("apartments.csv")
data$price <- as.numeric(data$price)
```


## Function to determine which variable has the lowest P-Value

```{r}
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
```




--------------------------------------------------------------------
