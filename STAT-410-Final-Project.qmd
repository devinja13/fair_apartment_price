---
title: "Determining a Fair Apartment Price"
subtitle: "Stat 410 Final Project"
author: "Jack Dzialo, Devin Abraham, Tom Vincent"
format: pdf
---

## Purpose

### Why choose our dataset?
-   Aimed to address an actual problem that we face
-   Currently looking for housing next year

How to determine if price is fair?

## Proposal

-   Chose to use Forward Stepwise Search using F tests
    -   Tested interaction terms once individual predictors were determined
-   Read up upon different ways to determine model accuracy
-   Use confidence intervals generated by the model to see if posted price for an apartment is fair

## Definitions

1.  $AIC$ Measures trade off between model fit and complexity with a penalty for more complexity
2.  $AIC_c$ Corrected version of AIC with a larger penalty for regressors
3.  $BIC$ Penalizes model complexity heavily and favors simple model with large sample sizes
4.  $PRESS$ Predicted Residual Error Sum of Squares; predicts how accurate a model is at predicting data it hasn't seen before

## Data Collection

Dataset of apartments for rent, sourced from UCI dataset repository (https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified)52,000 rows, each representing a different apartment
22 columns, each representing a descriptor for an apartment listing Square feet, apartment type, number of bathrooms, has photos

```{r fig.width=8, fig.height=6, echo=FALSE}
data <- read.csv2("~/Downloads/apartments.csv")
data$price <- as.numeric(data$price)
hist(data$price,
     freq = FALSE,
     breaks = "scott",
     main = "Histogram of Price",
     xlab = "Price (dollars)",
     col = "lightblue",
     xlim = c(0, 10000))
```

##  Data Synopsis for All Variables
```{r data, echo=FALSE}
table <- data.frame(
  Statistic = c("Mean Price", "Max Price", "Min Price", "Median Price"),
  Value = c(round(mean(data$price, na.rm = TRUE)),
            round(max(data$price, na.rm = TRUE)),
            round(min(data$price, na.rm = TRUE)),
            round(median(data$price, na.rm = TRUE))
))
knitr::kable(table)

data$square_feet <- as.numeric(data$square_feet)

table <- data.frame(
  Statistic = c("Mean Square Feet", "Max Square Feet", "Min Square Feet", "Median Square Feet"),
  Value = c(round(mean(data$square_feet, na.rm = TRUE)),
            round(max(data$square_feet, na.rm = TRUE)),
            round(min(data$square_feet, na.rm = TRUE)),
            round(median(data$square_feet, na.rm = TRUE))
))
knitr::kable(table)

data$bedrooms <- as.numeric(data$bedrooms)

table <- data.frame(
  Statistic = c( "Max Bedrooms", "Min Bedrooms"),
  Value = c(
            max(data$bedrooms, na.rm = TRUE),
            min(data$bedrooms, na.rm = TRUE)
))
knitr::kable(table)

data$bathrooms <- as.numeric(data$bathrooms)


table <- data.frame(
  Statistic = c("Max Bathrooms", "Min Bathrooms"),
  Value = c(
            max(data$bathrooms, na.rm = TRUE),
            min(data$bathrooms, na.rm = TRUE)
            
))
knitr::kable(table)
```


## Data Collection

The plot below shows each of the continuous predictors plotted against each other, in order to determine if multicolinearity exists.


# Data Analysis - Model Building

## Forward Stepwise Search

### First Iteration

For the first iteration of our stepwise search, we start off with a linear model that only consists of an intercept, $\beta_0$.

$$
Y = \beta_0
$$

After fitting the basic model with every predictor, we find that the Square feet variable results in the lowest P value of the model.

```{r, echo=FALSE}
library(ggplot2)
library(data.table)
library(dplyr)
library(stringr)
library(car)
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("~/Downloads/uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
regressor_list <- c("bedrooms", "bathrooms", "fee", "has_photo",
                    "pets_allowed", "square_feet",
                    "population", "density")
new <- regression(regressor_list)
table <- data.frame(
  Statistic = c("Predictor", "P-Value", "Adjusted R Squared", "AIC", "AICc", "BIC", "PRESS"), Values = new)
knitr::kable(table)
```

## Forward Stepwise Search {auto-animate="true"}

### Second Iteration
For the second iteration of our stepwise search, we use the linear model from the previous iteration, being the model with an intercept $\beta_0$ and a single predictor square feet, denoted as $X_1$.


$$
Y = \beta_0 + X_1\beta_1
$$

After fitting the model with every predictor, we find that the Density variable results in the lowest P value of the model.


## Forward Stepwise Search

### Second Iteration

The model improved when including density, while $AIC, AICc, BIC, PRESS$ got smaller.

## Forward Stepwise Search

### Third Iteration
For the third iteration, we now include both predictors: Square Feet (denoted by $X_1$), and density (denoted by $X_2$).

$Model:$

$$
Y = \beta_0 + X_1\beta_1 + X_2\beta_2
$$

After fitting the model with every predictor, we find that the Population variable results in the lowest P value of the model.

## Forward Stepwise Search

### Third Iteration


## Forward Stepwise Search

### Completed


Hence, we have found our linear model, with the predictor variables being Square feet ($X_1$), and denstiy ($X_2$).

$$
Y=\beta_0 + X_1\beta_1 + X_2\beta_2
$$

$32\%$ of the variation in price explained by model

```{r}
cleaned$price1 <- log(cleaned$price)
model <- lm(price ~ square_feet + density, data=cleaned)
summary_model <- summary(model)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```


# Analysis of Residuals - Model Improvement

## Residuals

### Residuals vs. Fitted Values

```{r}
model <- lm(price ~ square_feet + density, data=cleaned)
plot(model, which = 1)
```

Slight heteroskedasticity

## Residuals

### Quantile-Quantile

```{r}
plot(model, which = 2)
```

Relatively normal, room for improvement

## Residuals

### Scale-Location

```{r}
plot(model, which = 3)
```

Another heteroskedasticity indicator

## Residuals

### Residuals vs. Leverage

```{r}
plot(model, which = 5)
```

Multiple high leverage points

## Adjusted Model

### Outliers and Data Transformation

Address non-normality through Box-Cox transformation

```{r}
boxCox(model)
```

-   Optimal $\lambda$ close to 0

$$
y(\lambda) =
\begin{cases}
\frac{y^\lambda - 1}{\lambda}, & \text{if } \lambda \neq 0 \\[10pt]
\ln(y), & \text{if } \lambda = 0
\end{cases}
$$

## Adjusted Model

### Summary

Our adjusted model is: $\text{ln}(Y)=\beta_0+X_1\beta_1+X_2\beta_2$

```{r}
cleaned_new <- cleaned[-c(6754, 20556, 18779, 29745), ]
cleaned_new1 <- cleaned_new[-15668, ]
model_new <- lm(log(price) ~ square_feet + density, data=cleaned_new1)
summary_model <- summary(model_new)
table <- data.frame(
  Statistic = c("Intercept", "Square Feet", "Density"),
  Values = c(round(summary_model$coefficients[1,1], 4),round(summary_model$coefficients[2,1], 4),round(summary_model$coefficients[3,1], 4)),
  SE = c(round(summary_model$coefficients[1,2],4),round(summary_model$coefficients[2,2], 4),round(summary_model$coefficients[3,2],4)),
  P_Value = c(summary_model$coefficients[1,4],summary_model$coefficients[2,4],summary_model$coefficients[3,4]),
  R_Squared = c(round(summary_model$r.squared,4), "", "")
)
  knitr::kable(table, caption = "Summary of Model")
```


## Adjusted Model Residuals

### Residuals vs. Fitted Values

**Previous**

```{r fig.height=8, fig.width=10}
plot(model, which = 1)
```

**Adjusted**

```{r fig.height=8, fig.width=10}
plot(model_new, which = 1)
```

Normallity improved slightly

## Adjusted Model Residuals

### Quantile-Quantile

**Previous**

```{r fig.height=8, fig.width=10}
plot(model, which = 2)
```
**Adjusted**

```{r fig.height=8, fig.width=10}
plot(model_new, which = 2)
```

Significant Improvment 


## Adjusted Model Residuals

### Scale-Location


**Previous**

```{r fig.height=8, fig.width=10}
plot(model, which = 3)
```

**Adjusted**

```{r fig.height=8, fig.width=10}
plot(model_new, which = 3)
```

Slight homoscedasticity

## Adjusted Model Residuals

### Residuals vs. Leverage

**Previous**

```{r fig.height=8, fig.width=10}
plot(model, which = 5)
```

**Adjusted**

```{r fig.height=8, fig.width=10}
plot(model_new, which = 5)
```

Less high leverage points/outliers

## Confidence Interval

### Real Apartment Listing

The confidence interval generated by the model:

```{r}
calculated_density <- cities %>% filter(city == "houston", state_id == "TX") %>% select(density)
predictions <- predict(model_new, data.frame(square_feet = c(1200), density = c(2*calculated_density)), interval = "confidence", level = 0.95)
knitr::kable(exp(predictions))
```


-   Density metric was taken from the dataset
-   Scaled to account for higher density


Good Prediction

## Summary


-   Explored the data and found 8 usable regressors 5 of which were continuous and 3 of which were categorical
-   We used Forward Step wise Search to build our model and found 2 regressors for our final model, square_feet and density
-   Adjusted our model to take the log of price which improved $R^2$

------------------------------------------------------------------------

Limitations


-   Since Forward Step Wise Search is a greedy algorithm so it makes the best choice at each step and does not guarentee the most optimal solution
-   In the future we would like a data set with more regressors, our group hypothesizes that a regressor that represents the apartments luxuriousness would have a significant impact
    -   Could also use interaction terms to account for nonlinear effects



##Appendix

#Histogram

```{r fig.width=8, fig.height=6 }
data <- read.csv2("~/Downloads/apartments.csv")
data$price <- as.numeric(data$price)
```


## Function to determine which variable has the lowest P-Value

```{r}
cleaned <- data %>%
              filter(currency == "USD", price_type == "Monthly", nchar(state) == 2, bathrooms != "null", !bedrooms %in% c("null", "USD", "") ) %>%
              mutate(pets_allowed = ifelse(pets_allowed %in% c("Cats", "Dogs", "Cats, Dogs", "Cats, Dogs, None"), 1, 0)) %>%
              mutate(has_photo = ifelse(has_photo %in% c("Thumbnail", "Yes"), 1, 0)) %>%
              mutate(fee = ifelse(fee == "Yes", 1, 0))
cities <- read.csv("~/Downloads/uscities.csv")
cleaned <- cleaned %>%
  mutate(city = tolower(cityname), state = tolower(state))
cities <- cities %>%
  mutate(city = tolower(city), state = tolower(state_id))
cleaned <- cleaned %>%
  mutate(city = trimws(city), state = trimws(state))
cities <- cities %>%
  mutate(city = trimws(city), state = trimws(state))
data <- cleaned %>%
  left_join(cities %>% select(city, state, population, density),
            by = c("city", "state"))
#can do either median or mean.
pop_mean <- (mean(cities$population, na.rm=TRUE))
density_med <- mean(cities$density, na.rm=TRUE)
cleaned <- mutate(data, population = ifelse(is.na(population), pop_mean, population), density = ifelse(is.na(density), density_med, density))
Y <- cleaned$price
cleaned$bathrooms <- as.numeric(cleaned$bathrooms)
cleaned$bedrooms <- as.numeric(cleaned$bedrooms)
cleaned$price <- as.numeric(cleaned$price)
cleaned$density <- as.numeric(cleaned$density)
cleaned$population <- as.numeric(cleaned$population)
cleaned$square_feet <- as.numeric(cleaned$square_feet)
regression <- function(regressors, add_col = NULL) {
  low_pval <- 10
  best_var <- NULL
  adj_r2 <- 0
  best_var_sum <- NULL
  high_t_val <- 0
  AIC <- NULL
  AICc <- NULL
  BIC <- NULL
  # Iterate through the regressors
  for (i in regressors) {
    # Dynamically create the formula
    if (is.null(add_col) || length(add_col) == 0) {
      formula <- reformulate(i, response = "price")
    } else {
      formula <- reformulate(c(add_col, i), response = "price")
    }
    # Fit the linear model
    model <- invisible(lm(formula, data = cleaned))
    sum <- summary(model)
    # Extract the p-value for the slope (the last row of coefficients)
    # Ensure the slope term exists
    if (nrow(sum$coefficients) > 1) {
      pvalue <- sum$coefficients[nrow(sum$coefficients), "Pr(>|t|)"]
      tvalue <- abs(sum$coefficients[nrow(sum$coefficients), "t value"])
      # Update the lowest p-value and corresponding variable
      if (tvalue > high_t_val) {
        low_pval <- pvalue
        best_var <- i
        adj_r2 <- sum$adj.r.squared
        best_var_sum <- sum
        high_t_val <- tvalue
        AIC <- extractAIC(model, k=2)[2]
        npar <- length(sum$coefficients) + 1;
        n <- length(sum$residuals)
        AICc <- AIC + 2*npar*(npar + 1) / (n - npar - 1)
        BIC <- extractAIC(model, k = log(n))[2]
  residuals <- residuals(model)
  leverage <- hatvalues(model)
  press_statistic <- sum((residuals / (1 - leverage))^2)
      }
    }
  }
  # Return the variable with the lowest p-value
  return(c(best_var, low_pval, adj_r2, AIC, AICc, BIC, press_statistic))
}
```






